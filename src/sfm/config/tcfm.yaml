# @package _global_
# this @package directive solves any nesting problem (if this file is included in another folder)

defaults:
  # - ./source.normal
  - source: normal # normal, 8gaussian
  - model: mlp
  - _self_
  - data: moons

task: all # all, train, gif, sidebyside

datadim: 2

n_trainsteps: 10000
batch_size: 256
eval_batch_size: 2048
plot_batch_size: 1024
plot_integration_steps: 11 # for gif and sidebyside
eval_integration_steps: 100 # for training

# for testing limits of integration plot_int_steps.py
plot_maxint_steps: 5
# number of integration steps to plot side by side
plot_nint_steps: 5

# for conditional image generation like MNIST
# plot_batch_size should be at least plot_nrows**2
plot_nrows: 10

loss_scale: 1
lr: 0.001

# cfm or lipman
# Tong et al. 2024 style conditional flow matching
# Lipman et al. 2023 style target OT conditional flow matching
fmloss: cfm
use_slcf: true # True works better
use_ot: true
sigma: 0.01

# for some reason does not converge well
# maybe numerical instability?
# plim: [-0.5,1.5] # domain for plots
# source:
#   dmin: 0 # domain for data
#   dmax: 1

plim: [-7,7] # domain for plots
data:
  dmin: -5 # domain for data
  dmax: 5

source:
  data_dim: 2

# computational
force_cpu: false
savedir: null
runname: null

# log-prob, quiver (velocity field), trajectory
doplot: [False, False, True]

dpi: 150

# force retraining even if the model already exists
force_retrain: false

# logging
logger: null # neptune, wandb
tags: null
logfreq: 100
evalfreq: 1000
seed: 42
dtype: float32

wandb_run_name: FM
# variables we can access in our code
job_name: 'results'
# job_name: ${hydra:job.name}
config_name: ${hydra:job.config_name}
# Stores the command line arguments overrides
override_dirname: ${hydra:job.override_dirname}